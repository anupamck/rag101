{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026108df-4850-4372-bda1-e7b41d5dfbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: /Users/anupam/Documents/Programming/rag101/.env\n",
      "Keys in .env: ['HANDBOOK_SOURCE', 'LANGSMITH_API_KEY', 'LANGSMITH_ENDPOINT', 'LANGSMITH_PROJECT', 'LANGSMITH_TRACING', 'OPENAI_API_KEY', 'POSTS_SOURCE']\n",
      "Has OPENAI_API_KEY in .env?: True\n",
      "Env OPENAI_API_KEY present?: True\n",
      "Value prefix (masked): sk-pro…\n",
      "cwd: /Users/anupam/Documents/Programming/rag101\n"
     ]
    }
   ],
   "source": [
    "## Load environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv, dotenv_values\n",
    "\n",
    "# Load with explicit path and allow override\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "print(\"dotenv_path:\", dotenv_path or \"NOT FOUND\")\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "# Show what was parsed from the file (safe preview)\n",
    "parsed = dotenv_values(dotenv_path) if dotenv_path else {}\n",
    "print(\"Keys in .env:\", sorted(parsed.keys()))\n",
    "print(\"Has OPENAI_API_KEY in .env?:\", \"OPENAI_API_KEY\" in parsed)\n",
    "\n",
    "val = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Env OPENAI_API_KEY present?:\", val is not None)\n",
    "print(\"Value prefix (masked):\", (val[:6] + \"…\") if val else None)\n",
    "\n",
    "# Current working directory (to catch path mistakes)\n",
    "print(\"cwd:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5070809-d85c-49dc-bf5e-e525abed981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM model\n",
    "\n",
    "import getpass, os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e3e4da-a9e4-4009-b017-1c9f58eef03c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose embeddings\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72129c7-a4d4-4170-b60f-728358d64a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose vector store\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed12328-7bb8-4a2c-9372-c5e59784c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading handbook...\n",
      "Loaded 17 handbook entries\n",
      "Converting handbook sections to Langchain documents...\n",
      "Created 17 documents\n",
      "Document Ids: ['13edb3bc-6198-4963-a8c8-1289f08d78bb', '566d03c5-cc58-4d45-a588-63cfc3e61986', '044a934f-0a7b-4ff9-a2ba-74f1aca40ef8', '0ff32a91-6311-4821-82f0-595fc159a8fc', '981b1a9b-b790-4000-bac6-a74298fa0bdd']\n"
     ]
    }
   ],
   "source": [
    "# Chunk handbook and load into Langchain as documents\n",
    "\n",
    "from typing import TypedDict, List, Dict\n",
    "import json, os\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class HandbookEntry(TypedDict):\n",
    "    url: str\n",
    "    title: str\n",
    "    sections: Dict[str, str]\n",
    "\n",
    "def load_handbook(json_path: str) -> List[HandbookEntry]:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_documents(entries: List[HandbookEntry]) -> List[Document]:\n",
    "    documents = []\n",
    "    \"\"\"\n",
    "    # chunk each section within article individually\n",
    "    for entry in entries:\n",
    "        for section_title, section_text in entry['sections'].items():\n",
    "            if not section_text:\n",
    "                continue\n",
    "            metadata = {\n",
    "                'url': entry['url'],\n",
    "                'title': entry['title'],\n",
    "                'section': section_title,\n",
    "                }\n",
    "            documents.append(Document(page_content=section_text, metadata=metadata))\n",
    "    \"\"\"\n",
    "    # chunk each article individually\n",
    "    for entry in entries:\n",
    "        metadata = {\n",
    "                'url': entry['url'],\n",
    "                'title': entry['title'],\n",
    "                }\n",
    "        article_text = \"\\n\\n\".join(f\"{section}\\n\\n{text}\" for section, text in entry[\"sections\"].items())\n",
    "        documents.append(Document(page_content=article_text, metadata=metadata))    \n",
    "    return documents\n",
    "\n",
    "print(\"Loading handbook...\")\n",
    "handbook_entries = load_handbook(os.environ.get(\"HANDBOOK_SOURCE\"))\n",
    "print(f\"Loaded {len(handbook_entries)} handbook entries\")\n",
    "\n",
    "# Convert to Langchain documents (one per section)\n",
    "print(\"Converting handbook sections to Langchain documents...\")\n",
    "documents = create_documents(handbook_entries)\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "\n",
    "# Index documents\n",
    "document_ids = vector_store.add_documents(documents=documents)\n",
    "print(\"Document Ids:\", document_ids[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb89cd05-48cb-4248-8db9-130fa40193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing_extensions import List, TypedDict\n",
    "import json\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "        Act as a conversational interface for answering questions based on the content of the handbook in your knowledge base.\n",
    "\n",
    "        When information related to a specific topic does not exist, return no results.\n",
    "                \n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\n",
    "        \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c97a3d-7ddd-4d34-b651-4936a6e00472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import json\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State, min_similarity: float = 0.10 , max_docs: int = 8):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search_with_score(state[\"question\"], k=max_docs)\n",
    "    # Filter by threshold; note: depending on backend, higher score can mean closer or farther.\n",
    "    # For Chroma + cosine similarity in LC, score is often distance; adjust comparator accordingly.\n",
    "    relevant = []\n",
    "    relevant_log = []\n",
    "    for doc, score in results:\n",
    "        if score >= min_similarity:\n",
    "            relevant.append(doc)\n",
    "            relevant_log.append(f\"Doc: {doc.metadata.get('title', 'Unknown')}\\nScore: {score}\")\n",
    "    print(\"\\n\\n\".join(relevant_log))\n",
    "    return {\"context\": relevant}\n",
    "    \"\"\"\n",
    "\n",
    "def generate_with_links(state: State):\n",
    "    if not state[\"context\"]:\n",
    "        \n",
    "        return {\"answer\": \"I don't know.\" + \"\\n\\nNo relevant documents found.\"}\n",
    "    \n",
    "    # Get the base answer\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    base_answer = response.content\n",
    "    \n",
    "    # Extract unique links from context\n",
    "    unique_links = {}\n",
    "    for doc in state[\"context\"]:\n",
    "        title = doc.metadata.get('title', 'Unknown')\n",
    "        url = doc.metadata.get('url', '')\n",
    "        if url and title not in unique_links:\n",
    "            unique_links[title] = url\n",
    "    \n",
    "    # Format links section\n",
    "    if unique_links:\n",
    "        links_section = \"\\n\\nRelevant documents posts:\\n\"\n",
    "        for title, link in unique_links.items():\n",
    "            links_section += f\"- [{title}]({link})\\n\"\n",
    "        \n",
    "        final_answer = base_answer + links_section\n",
    "    else:\n",
    "        final_answer = base_answer + \"\\n\\nNo relevant documents found.\"\n",
    "    \n",
    "    return {\"answer\": final_answer}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate_with_links])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355fbde5-34e6-4934-9c3b-25d1ab247477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create evaluation dataset\n",
    "from typing import List, Dict\n",
    "\n",
    "def create_evaluation_dataset(question: str  ) -> List[Dict]:\n",
    "    \"\"\"Create evaluation dataset by running questions through the RAG system\"\"\"\n",
    "    \n",
    "    evaluation_data = []\n",
    "    # Get RAG response\n",
    "    response = graph.invoke({\"question\": question})\n",
    "    # Extract retrieved contexts (from the retrieve step)\n",
    "    retrieved_docs = response.get(\"context\", [])\n",
    "    retrieved_contexts = [doc.page_content for doc in retrieved_docs] if retrieved_docs else []\n",
    "    answer = response[\"answer\"]\n",
    "    evaluation_data.append({\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": retrieved_contexts,\n",
    "            \"response\": answer\n",
    "        })   \n",
    "    return evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a19cbb6f-106a-4c7e-8a0f-11680ee02f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Setup Ragas evaluation\n",
    "import gc\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, \n",
    "    Faithfulness, \n",
    "    FactualCorrectness,\n",
    "    AnswerRelevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    NonLLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    NonLLMContextRecall\n",
    "    \n",
    ")\n",
    "def perform_ragas_evaluation(evaluation_dataset_raw):\n",
    "    # Convert to Ragas format\n",
    "    evaluation_dataset = EvaluationDataset.from_list(evaluation_dataset_raw)\n",
    "    \n",
    "    # Setup evaluator LLM (using the same LLM for consistency)\n",
    "    evaluator_llm = LangchainLLMWrapper(llm)\n",
    "    \n",
    "    # Choose metrics (start with lighter ones to avoid memory issues)\n",
    "    metrics = [\n",
    "        AnswerRelevancy(),      # How relevant is the answer to the question\n",
    "        Faithfulness(),         # Is the answer faithful to the retrieved context\n",
    "        LLMContextPrecisionWithoutReference(), \n",
    "        # LLMContextPrecisionWithReference(),\n",
    "        # NonLLMContextPrecisionWithReference(),\n",
    "        # LLMContextRecall(),\n",
    "        # NonLLMContextRecall(),\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting Ragas evaluation...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Add garbage collection before evaluation\n",
    "    gc.collect()\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=evaluator_llm\n",
    "    )\n",
    "    print(\"Evaluation completed!\")\n",
    "    print(f\"Results: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bffebf03-5466-4b2e-8205-69cc260e476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation dataset...\n",
      "Here is the response: \n",
      "\n",
      " You can work on another job in parallel, but there are specific guidelines to follow. Occasional side gigs, speaking engagements, or advisory roles are generally acceptable as long as they don't conflict with your responsibilities at 37signals or require significant time commitments. However, working full-time or part-time for another company in the same industry is not allowed, and any side work should not interfere with your performance or dedication to your role at 37signals. If you're unsure about a specific situation, it's best to reach out to your manager for clarification.\n",
      "\n",
      "Relevant documents posts:\n",
      "- [A Note About Moonlighting](https://basecamp.com/handbook/moonlighting)\n",
      "- [How We Work](https://basecamp.com/handbook/how-we-work)\n",
      "- [Making a Career](https://basecamp.com/handbook/making-a-career)\n",
      "- [Getting Started](https://basecamp.com/handbook/getting-started)\n",
      "\n",
      "Starting Ragas evaluation...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f18dd4c136b4ff2ad4343d0eb688b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed!\n",
      "Results: {'answer_relevancy': 0.0000, 'faithfulness': 0.8750, 'llm_context_precision_without_reference': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "# Sample question for evaluation\n",
    "sample_question = \"Can I work on another job in parallel?\"\n",
    "\n",
    "print(\"Creating evaluation dataset...\")\n",
    "evaluation_dataset_raw = create_evaluation_dataset(sample_question)\n",
    "print('Here is the response: \\n\\n', evaluation_dataset_raw[0]['response']) \n",
    "perform_ragas_evaluation(evaluation_dataset_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d66d7-51c6-4b5c-84c4-f5ddd610540d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
